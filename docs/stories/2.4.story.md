# Story 2.4: Flow Execution Endpoint with Basic Orchestration

## Status
**In Progress** ðŸ”„

## Story
**As a** developer,
**I want** a /run endpoint that executes agent flows with provided input,
**so that** users can trigger agent processing via REST API.

## Acceptance Criteria
1. POST /run accepts JSON with flow (default "default"), input (prompt text), and story_id
2. Endpoint triggers sequential execution of agents in specified flow
3. Response returns either completed results or {"pending_docs": [...]} if documents missing
4. Memory context properly initialized with story_id for agent isolation
5. Execution errors caught and returned as 500 responses with error details
6. Request validation ensures required fields present
7. Async execution prevents blocking other requests
8. Integration test verifies basic flow execution with mock agents

## Tasks / Subtasks
- [ ] Task 1: Create /run endpoint with request/response models (AC: 1, 6)
  - [ ] Define Pydantic models for RunRequest and RunResponse
  - [ ] Implement POST /run endpoint in app.py
  - [ ] Add validation for required fields (flow, input, story_id)
  - [ ] Set default flow to "default" if not provided
- [ ] Task 2: Implement basic flow executor (AC: 2, 7)
  - [ ] Create executor.py module with FlowExecutor class
  - [ ] Load generated agents dynamically from /generated/agents
  - [ ] Implement sequential agent execution using PocketFlow
  - [ ] Ensure async execution throughout for non-blocking I/O
- [ ] Task 3: Add document dependency checking (AC: 3)
  - [ ] Check for required documents before agent execution
  - [ ] Build pending_docs list if any documents missing
  - [ ] Return early with pending_docs response if incomplete
  - [ ] Continue execution only when all dependencies satisfied
- [ ] Task 4: Initialize memory context (AC: 4)
  - [ ] Create memory context with story_id before flow starts
  - [ ] Pass memory manager to agents for get/set operations
  - [ ] Ensure proper isolation using story_id scoping
  - [ ] Flush memory cache after flow completion
- [ ] Task 5: Implement error handling (AC: 5)
  - [ ] Wrap execution in try/except blocks
  - [ ] Catch and log execution errors with details
  - [ ] Return 500 status with error message on failure
  - [ ] Ensure partial results not returned on error
- [ ] Task 6: Create integration tests (AC: 8)
  - [ ] Mock agent modules for testing
  - [ ] Test successful flow execution
  - [ ] Test pending_docs response
  - [ ] Test error handling scenarios
  - [ ] Verify async behavior and non-blocking execution

## Dev Notes

### Previous Story Context
Story 2.3 implemented memory storage with:
- MemoryManager with get/set operations and scope-based isolation
- File-based JSONL backend with caching
- Thread-safe operations with asyncio locks
- REST endpoints for memory debugging

This story builds on the memory system to provide actual flow execution capabilities.

### PocketFlow Integration
[Source: pocketflow/__init__.py, CLAUDE.md#pocketflow-learning-resources]

The execution should follow PocketFlow patterns:
1. **Use Flow class** for orchestration, not manual execution
2. **Node lifecycle**: prep â†’ exec â†’ post with shared store
3. **Reference cookbook examples**:
   - `cookbook/pocketflow-agent/` for basic agent pattern
   - `cookbook/pocketflow-workflow/` for flow orchestration
   - `cookbook/pocketflow-fastapi-background/` for async execution

### KISS Implementation Pattern
[Source: CLAUDE.md - KISS principle, docs/architecture/coding-standards.md]

Keep the implementation simple - basic sequential execution first:
```python
from pocketflow import Flow, Node
from typing import Dict, List, Optional
from pydantic import BaseModel
import importlib

class RunRequest(BaseModel):
    flow: str = "default"
    input: str
    story_id: str

class RunResponse(BaseModel):
    result: Optional[str] = None
    pending_docs: Optional[List[str]] = None
    
class FlowExecutor:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
        self.agents = self._load_agents()
    
    def _load_agents(self) -> Dict[str, Node]:
        """Dynamically load agent modules from /generated/agents."""
        agents = {}
        # Simple dynamic import of agent modules
        # Each agent should be a PocketFlow Node subclass
        return agents
    
    async def execute(self, request: RunRequest) -> RunResponse:
        """Execute flow with given input and story_id."""
        # 1. Check document dependencies
        # 2. Initialize memory context
        # 3. Build and run PocketFlow Flow
        # 4. Return results or pending_docs
        pass
```

### API Endpoint Implementation
[Source: docs/architecture/tech-stack.md#application-framework - FastAPI patterns]

```python
from fastapi import APIRouter, HTTPException
from .executor import FlowExecutor, RunRequest, RunResponse

router = APIRouter(tags=["execution"])

@router.post("/run", response_model=RunResponse)
async def run_flow(request: RunRequest):
    """Execute an agent flow with provided input."""
    try:
        executor = FlowExecutor(memory_manager)
        response = await executor.execute(request)
        return response
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Flow execution failed: {str(e)}")
```

### Agent Loading Pattern
[Source: cookbook/pocketflow-agent/nodes.py - Agent node structure]

Generated agents should follow this pattern:
```python
from pocketflow import Node

class AnalystAgent(Node):
    def prep(self, shared):
        # Read input from shared store
        return {"input": shared.get("input")}
    
    def exec(self, prep_res):
        # Execute agent logic (LLM call, etc.)
        # This is where BMAD prompt gets used
        return {"output": "analysis result"}
    
    def post(self, shared, prep_res, exec_res):
        # Write results to shared store
        shared["analyst_output"] = exec_res["output"]
        return "complete"  # Action for flow control
```

### Document Dependency Checking
[Source: docs/architecture.md#document-contract]

Before execution, check all required documents exist:
```python
from pathlib import Path

async def check_documents(required_docs: List[str]) -> List[str]:
    """Check which documents are missing."""
    missing = []
    for doc_id in required_docs:
        doc_path = Path(f"docs/{doc_id}.md")
        if not doc_path.exists():
            missing.append(doc_id)
    return missing
```

### Memory Context Pattern
[Source: Story 2.3 implementation - generated/memory.py]

Initialize memory for the flow:
```python
async def init_memory_context(memory_manager, story_id: str, agent_id: str):
    """Initialize memory context for agent execution."""
    # Memory key pattern for isolated scope: {agent_id}:{story_id}
    context_key = f"{agent_id}:{story_id}"
    
    # Load existing memory or create new
    existing = await memory_manager.get("isolated", context_key)
    if not existing:
        await memory_manager.set("isolated", context_key, {
            "story_id": story_id,
            "agent_id": agent_id,
            "status": "initialized"
        })
    
    return context_key
```

### Testing Strategy
[Source: docs/architecture/coding-standards.md#testing-standards]

- **Test File**: tests/integration/test_flow_execution.py
- **Mock Agents**: Create simple mock agents that return predictable results
- **Test Scenarios**:
  - Successful flow with all dependencies present
  - Missing documents returning pending_docs
  - Error during agent execution
  - Memory isolation between different story_ids
  - Concurrent flow executions

Example test:
```python
import pytest
from httpx import AsyncClient
from unittest.mock import patch, MagicMock

@pytest.mark.asyncio
async def test_run_flow_success(client: AsyncClient):
    """Test successful flow execution."""
    # Mock agent execution
    with patch("generated.executor.FlowExecutor._load_agents") as mock_load:
        mock_agent = MagicMock()
        mock_agent.run.return_value = {"result": "test output"}
        mock_load.return_value = {"test_agent": mock_agent}
        
        response = await client.post("/run", json={
            "flow": "default",
            "input": "test input",
            "story_id": "test_story_001"
        })
        
        assert response.status_code == 200
        assert response.json()["result"] == "test output"
        assert response.json()["pending_docs"] is None
```

### File Structure
[Source: docs/architecture/source-tree.md]
```
generated/
â”œâ”€â”€ app.py                   # FastAPI app (updated with /run endpoint)
â”œâ”€â”€ executor.py              # THIS STORY - Flow executor
â”œâ”€â”€ documents.py             # Document storage (from Story 2.2)
â”œâ”€â”€ memory.py                # Memory manager (from Story 2.3)
â”œâ”€â”€ memory_router.py         # Memory API (from Story 2.3)
â””â”€â”€ agents/                  # Generated agent modules
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ analyst.py          # Example generated agent
    â””â”€â”€ summarizer.py       # Example generated agent
```

### Dependencies
[Source: docs/architecture/tech-stack.md]
Already available from previous stories:
- pocketflow (core framework)
- fastapi (REST framework)
- pydantic (validation)
- asyncio (async execution)

May need to add:
- importlib (for dynamic agent loading - part of stdlib)

### Performance Considerations
[Source: docs/architecture.md#performance-architecture]
- Use async/await throughout to prevent blocking
- Load agents once on startup, not per request
- Keep memory cache warm during flow execution
- Consider connection pooling for LLM calls (future optimization)

### Security Considerations
[Source: docs/architecture/coding-standards.md#security-standards]
- Validate flow name against allowed flows
- Sanitize story_id to prevent injection
- Limit input size to prevent DoS
- Never execute arbitrary code from input

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-06 | 0.1 | Initial story creation | Bob (SM) |

## Dev Agent Record

### Implementation Summary
**Date**: [To be filled by dev agent]
**Agent**: [To be filled by dev agent]
**Duration**: [To be filled by dev agent]
**Status**: [To be filled by dev agent]

### Files Created/Modified
[To be filled by dev agent]

### Implementation Approach
[To be filled by dev agent]

### Key Design Decisions
[To be filled by dev agent]

### Test Results
[To be filled by dev agent]

### Architecture Compliance
[To be filled by dev agent]

## QA Results

### Review Date: [To be filled]

### Reviewed By: [To be filled]

### Code Quality Assessment
[To be filled]

### Refactoring Performed
[To be filled]

### Compliance Check
[To be filled]

### Improvements Checklist
[To be filled]

### Security Review
[To be filled]

### Performance Considerations
[To be filled]

### Test Coverage Analysis
[To be filled]

### Final Status
[To be filled]